{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac0a33e",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import pkuseg\n",
    "import csv\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10)\n",
    "    data = pd.read_csv('answer.csv', delimiter=',')\n",
    "    sample_portion = 0.01\n",
    "    sample_size = int(sample_portion * len(data['content']))\n",
    "    data_sample = data['content'].sample(n=sample_size)\n",
    "    seg = pkuseg.pkuseg(model_name='medicine')\n",
    "    # data = pd.read_csv('sampled_answer.csv', delimiter=',', header=None)[0]\n",
    "    data = data_sample\n",
    "    vocabulary = set(pd.read_csv('all_med_words.csv', delimiter='\\t', header=None)[0])\n",
    "    segmented_answer = [' '.join([word for word in seg.cut(text) if word in vocabulary]) for text in data]\n",
    "    segment_df = pd.DataFrame(segmented_answer)\n",
    "    segment_df.to_csv('segmented_answer.csv', index=False, header=False)\n",
    "    data = segment_df[0]\n",
    "    data = [i for i in data if len(i) > 0]\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "    model = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(data, padding=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "    length = len(sentence_embeddings)\n",
    "    output = []\n",
    "    for i in range(length):\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "        output += [(data[i], data[j], int(cosine_similarity(sentence_embeddings[i], sentence_embeddings[j]) // 0.2)) for j in range(length)]\n",
    "    labeled_df =  pd.DataFrame(output)\n",
    "    labeled_df = labeled_df[(labeled_df[2] == 4) | (labeled_df[2] < 1)]\n",
    "    labeled_df[2] = labeled_df[2] // 4\n",
    "    labeled_df = labeled_df.sample(n=24000, replace=True)\n",
    "    labeled_df.to_csv('cnn_data.csv', index=False)\n",
    "    embedding = KeyedVectors.load_word2vec_format('./models/Medical.txt', binary=False)\n",
    "    embedding.vectors = np.random.rand(embedding.vectors.shape[0], embedding.vectors.shape[1])\n",
    "    padding_num = len(embedding.vectors)\n",
    "    def collate_fn(data):\n",
    "        s0, s1, labels = zip(*data)\n",
    "        s0 = pad_sequence(s0, batch_first=True, padding_value=padding_num)\n",
    "        s1 = pad_sequence(s1, batch_first=True, padding_value=padding_num)\n",
    "        return s0, s1, torch.tensor(labels, dtype=torch.float).reshape((len(labels), 1))\n",
    "    random_vector_embedding = SentenceDataset(embedding)\n",
    "    split = int(len(random_vector_embedding)*0.8)\n",
    "    lengths = [split, len(random_vector_embedding) - split]\n",
    "    train_dataset, val_dataset = random_split(random_vector_embedding, lengths)\n",
    "    def load_data(train_dataset, val_dataset):\n",
    "\n",
    "        batch_size = 32\n",
    "        # your code here\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "\n",
    "    train_loader, val_loader = load_data(train_dataset, val_dataset)\n",
    "    model = SimilarityCNN(embedding)\n",
    "    n_epochs = 4\n",
    "    # load the loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    # load the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    model = train_model(model, train_loader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion)\n",
    "    acc, p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print(f'Validation acc: {acc}, p:{p}, f:{f}, roc_auc:{roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLH_project",
   "language": "python",
   "name": "dlh_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
